{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw features explainability\n",
    "\n",
    "We will be using the Titanic data set. So from the raw data that is a mixture of categoricals and numericals, we will featurize the categoricals using one hot encoding.\n",
    "\n",
    "Explain a model with the AML explain-model package on raw features:\n",
    "\n",
    "1. Train a Logistic Regression model using Scikit-learn\n",
    "2. Run 'explain_model' with full dataset in local mode, which doesn't contact any Azure services.\n",
    "3. Run 'explain_model' with summarized dataset in local mode, which doesn't contact any Azure services.\n",
    "4. Visualize the global and local explanations with the visualization dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using the Titanic dataset for this example\n",
    "data_url = (\n",
    "    \"https://raw.githubusercontent.com/amueller/\"\n",
    "    \"scipy-2017-sklearn/091d371/notebooks/datasets/titanic3.csv\"\n",
    ")\n",
    "data = pd.read_csv(data_url)\n",
    "\n",
    "# fill missing values\n",
    "data = data.fillna(method=\"ffill\")\n",
    "data = data.fillna(method=\"bfill\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model and train\n",
    "\n",
    "The numeric data is standard-scaled after median-imputation, while the categorical data is one-hot encoded after imputing missing values with a new category ('missing').\n",
    "\n",
    "Finally, the preprocessing pipeline is integrated in a full prediction pipeline using sklearn.pipeline.Pipeline, together with a simple classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We will train our classifier with the following features:\n",
    "# Numeric Features:\n",
    "# - age: float.\n",
    "# - fare: float.\n",
    "# Categorical Features:\n",
    "# - embarked: categories encoded as strings {'C', 'S', 'Q'}.\n",
    "# - sex: categories encoded as strings {'female', 'male'}.\n",
    "# - pclass: ordinal integers {1, 2, 3}.\n",
    "\n",
    "numeric_features = [\"age\", \"fare\"]\n",
    "categorical_features = [\"embarked\", \"sex\", \"pclass\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"survived\"].values\n",
    "X = data[categorical_features + numeric_features]\n",
    "\n",
    "# split the data in train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "# Impute, standardize the numeric features and one-hot encode the categorical features.\n",
    "# We create the preprocessing pipelines for both numeric and categorical data.\n",
    "\n",
    "transformations = [\n",
    "    (\n",
    "        [\"age\", \"fare\"],\n",
    "        Pipeline(\n",
    "            steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                (\"scaler\", StandardScaler()),\n",
    "            ]\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        [\"embarked\"],\n",
    "        Pipeline(\n",
    "            steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "                (\"encoder\", OneHotEncoder(sparse=False)),\n",
    "            ]\n",
    "        ),\n",
    "    ),\n",
    "    ([\"sex\", \"pclass\"], OneHotEncoder(sparse=False)),\n",
    "]\n",
    "\n",
    "\n",
    "# Append classifier to preprocessing pipeline.\n",
    "# Now we have a full prediction pipeline.\n",
    "clf = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", DataFrameMapper(transformations)),\n",
    "        (\"classifier\", LogisticRegression(solver=\"lbfgs\")),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Train a logistic regression  model, which is what we want to explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"model score: {clf.score(x_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the explain model package\n",
    "\n",
    "See https://docs.microsoft.com/en-us/azure/machine-learning/service/machine-learning-interpretability-explainability\n",
    "\n",
    "We will be using the tabular explainer. \n",
    "As such we will receive explanations in terms of the raw features before the transformation (rather than engineered features). If you skip this, the explainer provides explanations in terms of engineered features.\n",
    "\n",
    "The format of supported transformations is same as the one described in sklearn-pandas. In general, any transformations are supported as long as they operate on a single column and are therefore clearly one to many.\n",
    "\n",
    "\n",
    "---\n",
    "Meta explainers automatically select a suitable direct explainer and generate the best explanation info based on the given model and data sets. The meta explainers leverage all the libraries (SHAP, LIME, Mimic, etc.) that we have integrated or developed. The following are the meta explainers available in the SDK:\n",
    "\n",
    "- Tabular Explainer: Used with tabular datasets.\n",
    "- Text Explainer: Used with text datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.explain.model.tabular_explainer import TabularExplainer\n",
    "\n",
    "# Explain predictions on the local machine\n",
    "# clf.steps[-1][1] returns the trained classification model\n",
    "# Pass transformation as an input to create the explanation object\n",
    "# \"features\" and \"classes\" fields are optional\n",
    "\n",
    "tabular_explainer = TabularExplainer(\n",
    "    clf.steps[-1][1],\n",
    "    initialization_examples=x_train,\n",
    "    features=x_train.columns,\n",
    "    transformations=transformations,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing in test dataset for evaluation examples - note it must be a representative sample of the original data\n",
    "# x_train can be passed as well, but with more examples explanations will take longer although they may be more accurate\n",
    "\n",
    "global_explanation = tabular_explainer.explain_global(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the global importance of the features in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_global_importance_values = global_explanation.get_ranked_global_values()\n",
    "sorted_global_importance_names = global_explanation.get_ranked_global_names()\n",
    "dict(zip(sorted_global_importance_names, sorted_global_importance_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain overall model predictions as a collection of local (instance-level) explanations \n",
    "You can apply the interpretability classes and methods to understand the modelâ€™s global behavior or specific predictions. The former is called global explanation and the latter is called local explanation.\n",
    "\n",
    "So for this we will explain the first member of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_explanation = tabular_explainer.explain_local(x_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the prediction for the first member of the test set and explain why model made that prediction\n",
    "prediction_value = clf.predict(x_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_local_importance_values = local_explanation.get_ranked_local_values()[prediction_value]\n",
    "sorted_local_importance_names = local_explanation.get_ranked_local_names()[prediction_value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel Explainer: SHAP's Kernel explainer uses a specially weighted local linear regression to estimate SHAP values for any model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted local SHAP values\n",
    "print('ranked local importance values: {}'.format(sorted_local_importance_values))\n",
    "# Corresponding feature names\n",
    "print('ranked local importance names: {}'.format(sorted_local_importance_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load visualization dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.contrib.explain.model.visualize import ExplanationDashboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExplanationDashboard(global_explanation, model, x_test)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
